{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Concepts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCiKHALJa7G5"
      },
      "source": [
        "## ML Concepts\n",
        "\n",
        "> This is an ongoing repository built in order to understand few key concepts in Machine Learning, which any Data Scientist should be aware of.\n",
        " The articles are from resources which i ahve read/watched and understood, so I have consolidated them in this one article. The links of the original resources are provided\n",
        "\n",
        "Fundamental ML concepts\n",
        "\n",
        "### Understanding the Bias-Variance Tradeoff\n",
        "\n",
        "[link](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
        "\n",
        "When we discuss prediction models, prediction errors can be decomposed into two main subcomponents we care about: error due to \"bias\" and error due to \"variance\". There is a tradeoff between a model's ability to minimize bias and variance. Understanding these two types of error can help us diagnose model results and avoid the mistake of over- or under-fitting.\n",
        "\n",
        "- **Error due to Bias:** The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value.\n",
        "\n",
        "- **Error due to Variance:** The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.\n",
        "\n",
        "\n",
        "We can create a graphical visualization of bias and variance using a bulls-eye diagram. Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. Imagine we can repeat our entire model building process to get a number of separate hits on the target. Each hit represents an individual realization of our model, given the chance variability in the training data we gather. Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions. These different realizations result in a scatter of hits on the target.\n",
        "\n",
        "We can plot four different cases representing combinations of both high and low bias and variance.\n",
        "\n",
        "![](https://miro.medium.com/max/1200/1*CgIdnlB6JK8orFKPXpc7Rg.png)\n",
        "\n",
        "$$\n",
        "Err(x) = \\mathrm{Bias}^2 + \\mathrm{Variance} + \\mathrm{Irreducible\\ Error}\n",
        "$$\n",
        "\n",
        "That third term, irreducible error, is the noise term in the true relationship that cannot fundamentally be reduced by any model. Given the true model and infinite data to calibrate it, we should be able to reduce both the bias and variance terms to 0. However, in a world with imperfect models and finite data, there is a tradeoff between minimizing the bias and minimizing the variance.\n",
        "\n",
        "#### Bias Variance Trade off\n",
        "\n",
        "Increasing k results in the averaging of more voters in each prediction. This results in smoother prediction curves. With a k of 1, the separation between Democrats and Republicans is very jagged. Furthermore, there are \"islands\" of Democrats in generally Republican territory and vice versa. As k is increased to, say, 20, the transition becomes smoother and the islands disappear and the split between Democrats and Republicans does a good job of following the boundary line. As k becomes very large, say, 80, the distinction between the two categories becomes more blurred and the boundary prediction line is not matched very well at all.\n",
        "\n",
        "At small k's the jaggedness and islands are signs of variance. The locations of the islands and the exact curves of the boundaries will change radically as new data is gathered (**overfitting**) - low bias high variance, the model is very complex. On the other hand, at large k's the transition is very smooth so there isn't much variance, but the lack of a match to the boundary line is a sign of high bias (**underfitting**) - high bias low variance (imagine a simple st line separating the 2 groups).\n",
        "\n",
        "What we are observing here is that increasing k will decrease variance and increase bias. While decreasing k will increase variance and decrease bias. Take a look at how variable the predictions are for different data sets at low k. As k increases this variability is reduced. But if we increase k too much, then we no longer follow the true boundary line and we observe high bias. This is the nature of the Bias-Variance Tradeoff.\n",
        "\n",
        "#### Managing Bias and Variance\n",
        "\n",
        "Bagging and other resampling techniques can be used to reduce the variance in model predictions. In bagging (Bootstrap Aggregating), numerous replicates of the original data set are created using random selection with replacement. Each derivative data set is then used to construct a new model and the models are gathered together into an ensemble. To make a prediction, all of the models in the ensemble are polled and their results are averaged.\n",
        "\n",
        "One powerful modeling algorithm that makes good use of bagging is Random Forests. Random Forests works by training numerous decision trees each based on a different resampling of the original training data. In Random Forests the bias of the full model is equivalent to the bias of a single decision tree (which itself has high variance). By creating many of these trees, in effect a \"forest\", and then averaging them the variance of the final model can be greatly reduced over that of a single tree. In practice the only limitation on the size of the forest is computing time as an infinite number of trees could be trained without ever increasing bias and with a continual (if asymptotically declining) decrease in the variance.\n",
        "\n",
        "#### Understanding Over- and Under-Fitting\n",
        "\n",
        "At its root, dealing with bias and variance is really about dealing with over- and under-fitting. Bias is reduced and variance is increased in relation to model complexity. As more and more parameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. For example, as more polynomial terms are added to a linear regression, the greater the resulting model's complexity will be. In other words, bias has a negative first-order derivative in response to model complexity while variance has a positive slope.\n",
        "\n",
        "![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I8j3FZQpZU9"
      },
      "source": [
        "### KNN vs K means Clustering\n",
        "\n",
        "K-Nearest Neighbors is a supervised classification algorithm, while\n",
        "k-means clustering is an unsupervised clustering algorithm. While the\n",
        "mechanisms may seem similar at first, what this really means is that\n",
        "in order for K-Nearest Neighbors to work, you need labeled data you\n",
        "want to classify an unlabeled point into (thus the nearest neighbor\n",
        "part). K-means clustering requires only a set of unlabeled points and\n",
        "a threshold: the algorithm will take unlabeled points and gradually\n",
        "learn how to cluster them into groups by computing the mean of the\n",
        "distance between different points.\n",
        "\n",
        "The critical difference here is that KNN needs labeled points and is\n",
        "thus supervised learning, while k-means doesn’t — and is thus unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilRGj6y0u4CD"
      },
      "source": [
        "### Precision Recall F1 Score\n",
        "\n",
        "[link](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)\n",
        "\n",
        "![](https://miro.medium.com/max/490/1*TS2hsRr528UHQG9nDJhIcA.jpeg)\n",
        "\n",
        "You will notice that the accuracy for this model is very very high, at 99.9%!!\n",
        "What if I mentioned that the positive over here is actually someone who is sick and carrying a virus that can spread very quickly? Or the positive here represent a fraud case? Or the positive here represents terrorist that the model says its a non-terrorist? Well you get the idea. The costs of having a mis-classified actual positive (or false negative) is very high here in these three circumstances that I posed.\n",
        "\n",
        "![](https://miro.medium.com/max/355/1*7J08ekAwupLBegeUI8muHA.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/560/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg)\n",
        "\n",
        "Great! Now let us look at Precision first.\n",
        "\n",
        "What do you notice for the denominator? The denominator is actually the Total Predicted Positive! So the formula becomes\n",
        "\n",
        "![alt text](https://miro.medium.com/max/560/1*PULzWEven_XAZjiMNizDCg.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/355/1*C3ctNdO0mde9fa1PFsCVqA.png)\n",
        "\n",
        "> Immediately, you can see that Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.\n",
        "\n",
        "Precision is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.\n",
        "\n",
        "So let us apply the same logic for Recall. Recall how Recall is calculated.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/334/1*dXkDleGhA-jjZmZ1BlYKXg.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/560/1*BBhWQC-m0CLN4sVJ0h5fJQ.jpeg)\n",
        "\n",
        "> There you go! So Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.\n",
        "\n",
        "For instance, in fraud detection or sick patient detection. If a fraudulent transaction (Actual Positive) is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.\n",
        "Similarly, in sick patient detection. If a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative). The cost associated with False Negative will be extremely high if the sickness is contagious.\n",
        "\n",
        "\n",
        "#### F1 Score\n",
        "\n",
        "![alt text](https://miro.medium.com/max/226/1*T6kVUKxG_Z4V5Fm1UXhEIw.png)\n",
        "\n",
        "F1 Score is needed when you want to seek a balance between Precision and Recall. Right…so what is the difference between F1 Score and Accuracy then? We have previously seen that accuracy can be largely contributed by a large number of True Negatives which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible & intangible) thus F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n",
        "\n",
        "\n",
        "For the above ex:\n",
        "\n",
        "```\n",
        "TN = 998 TP = 1\n",
        "FP = 0 FN = 1\n",
        "\n",
        "So Precision = TP/(FP + TP) = 1\n",
        "\n",
        "Recall = TP/(FN + TP) = 1/(1+1) = 0.5\n",
        "\n",
        "F1 Score = 2*(1*0.5)/(1+0.5) = 0.66\n",
        "\n",
        "This is a considerably better evaluation\n",
        "```\n",
        "\n",
        "#### Another example\n",
        "\n",
        "[link](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2015/01/confusion_matrix2.png)\n",
        "\n",
        "This is a list of rates that are often computed from a confusion matrix for a binary classifier:\n",
        "\n",
        "- Accuracy: Overall, how often is the classifier correct?\n",
        "(TP+TN)/total = (100+50)/165 = 0.91\n",
        "- Misclassification Rate: Overall, how often is it wrong?\n",
        "(FP+FN)/total = (10+5)/165 = 0.09 equivalent to 1 minus Accuracy also known as \"Error Rate\"\n",
        "- True Positive Rate: When it's actually yes, how often does it predict yes?\n",
        "TP/actual yes = 100/105 = 0.95 also known as \"Sensitivity\" or \"Recall\"\n",
        "- False Positive Rate: When it's actually no, how often does it predict yes?\n",
        "FP/actual no = 10/60 = 0.17\n",
        "- True Negative Rate: When it's actually no, how often does it predict no?\n",
        "TN/actual no = 50/60 = 0.83 equivalent to 1 minus False Positive Rate\n",
        "also known as \"Specificity\"\n",
        "- Precision: When it predicts yes, how often is it correct?\n",
        "TP/predicted yes = 100/110 = 0.91\n",
        "- Prevalence: How often does the yes condition actually occur in our sample?\n",
        "actual yes/total = 105/165 = 0.64\n",
        "\n",
        "**Null Error Rate:** This is how often you would be wrong if you always predicted the majority class. (In our example, the null error rate would be 60/165=0.36 because if you always predicted yes, you would only be wrong for the 60 \"no\" cases.) This can be a useful baseline metric to compare your classifier against."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouvVd3OAxHMz"
      },
      "source": [
        "### ROC and AUC curve\n",
        "\n",
        "[link](https://www.dataschool.io/roc-curves-and-auc-explained/)\n",
        "[visulaization link](http://www.navan.name/roc/)\n",
        "\n",
        "An ROC curve is a commonly used way to visualize the performance of a binary classifier, meaning a classifier with two possible output classes.\n",
        "\n",
        "For example, let's pretend you built a classifier to predict whether a research paper will be admitted to a journal, based on a variety of factors. The features might be the length of the paper, the number of authors, the number of papers those authors have previously submitted to the journal, et cetera. The response (or \"output variable\") would be whether or not the paper was admitted.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc02.PNG)\n",
        "\n",
        "The x-axis represents your predicted probabilities, and the y-axis represents a count of observations, kind of like a histogram. Let's estimate that the height at 0.1 is 10 pixels. This plot tells you that there were 10 papers for which you predicted an admission probability of 0.1, and the true status for all 10 papers was negative (meaning not admitted) since 0.1 lies entirely in blue region. There were about 50 papers for which you predicted an admittance probability of 0.3, and none of those 50 were admitted. There were about 20 papers for which you predicted a probability of 0.5, and half of those were admitted and the other half were not since half of the region lies in blue and the other half in red. There were 50 papers for which you predicted a probability of 0.7, and all of those were admitted (see diag below). And so on.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc04.PNG)\n",
        "\n",
        "Based on this plot, you might say that your classifier is doing quite well, since it did a good job of separating the classes. To actually make your class predictions, you might set your \"threshold\" at 0.5, and classify everything above 0.5 as admitted and everything below 0.5 as not admitted, which is what most classification methods will do by default. With that threshold, your accuracy rate would be above 90%, which is probably very good.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc05.PNG)\n",
        "\n",
        "Now let's pretend that your classifier didn't do nearly as well and move the blue distribution. You can see that there is a lot more overlap here, and regardless of where you set your threshold, your classification accuracy will be much lower than before.\n",
        "\n",
        "Now let's talk about the ROC curve that you see here in the upper left. So, what is an ROC curve? It is a plot of the True Positive Rate (on the y-axis) versus the False Positive Rate (on the x-axis) for every possible classification threshold.\n",
        "\n",
        "True Positive Rate: When it's actually yes, how often does it predict yes? \n",
        "False Positive Rate: When it's actually no, how often does it predict yes?\n",
        "Both the True Positive Rate and the False Positive Rate range from 0 to 1.\n",
        "\n",
        "\n",
        "#### Generating ROC Curve\n",
        "\n",
        "To see how the ROC curve is actually generated, let's set some example thresholds for classifying a paper as admitted.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc08.PNG)\n",
        "\n",
        "A threshold of 0.8 would classify 50 papers as admitted, and 450 papers as not admitted. The True Positive Rate would be the red pixels to the right of the line divided by all red pixels, or 50 divided by 250, which is 0.2. The False Positive Rate would be the blue pixels to the right of the line divided by all blue pixels, or 0 divided by 250, which is 0. Thus, we would plot a point at 0 on the x-axis, and 0.2 on the y-axis, which is right here.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc09.PNG)\n",
        "\n",
        "Let's set a different threshold of 0.5. That would classify 360 papers as admitted, and 140 papers as not admitted. The True Positive Rate would be 235 divided by 250, or 0.94. The False Positive Rate would be 125 divided by 250, or 0.5. Thus, we would plot a point at 0.5 on the x-axis, and 0.94 on the y-axis, which is right here.\n",
        "\n",
        "We've plotted two points, but to generate the entire ROC curve, all we have to do is to plot the True Positive Rate versus the False Positive Rate for all possible classification thresholds which range from 0 to 1. That is a huge benefit of using an ROC curve to evaluate a classifier instead of a simpler metric such as misclassification rate, in that an ROC curve visualizes all possible classification thresholds, whereas misclassification rate only represents your error rate for a single threshold. Note that you can't actually see the thresholds used to generate the ROC curve anywhere on the curve itself.\n",
        "\n",
        "Now, let's move the blue distribution back to where it was before. Because the classifier is doing a very good job of separating the blues and the reds, I can set a threshold of 0.6, have a True Positive Rate of 0.8, and still have a False Positive Rate of 0.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc10.PNG)\n",
        "\n",
        "> Therefore, a classifier that does a very good job separating the classes will have an ROC curve that hugs the upper left corner of the plot. Conversely, a classifier that does a very poor job separating the classes will have an ROC curve that is close to this black diagonal line. That line essentially represents a classifier that does no better than random guessing.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc11.PNG)\n",
        "\n",
        "#### AUC\n",
        "\n",
        "Naturally, you might want to use the ROC curve to quantify the performance of a classifier, and give a higher score for this classifier than this classifier. That is the purpose of AUC, which stands for Area Under the Curve. AUC is literally just the percentage of this box that is under this curve. This classifier has an AUC of around 0.8, a very poor classifier has an AUC of around 0.5, and this classifier has an AUC of close to 1.\n",
        "\n",
        "![](https://www.dataschool.io/content/images/2014/11/roc12.PNG)\n",
        "\n",
        "There are two things I want to mention about this diagram. \n",
        "\n",
        "- First, this diagram shows a case where your classes are perfectly balanced, which is why the size of the blue and the red distributions are identical. In most real-world problems, this is not the case. For example, if only 10% of papers were admitted, the blue distribution would be nine times larger than the red distribution. However, that doesn't change how the ROC curve is generated.\n",
        "\n",
        "- A second note about this diagram is that it shows a case where your predicted probabilities have a very smooth shape, similar to a normal distribution. That was just for demonstration purposes. The probabilities output by your classifier will not necessarily follow any particular shape.\n",
        "\n",
        "\n",
        "To close, I want to add three other important notes. The first note is that the ROC curve and AUC are insensitive to whether your predicted probabilities are properly calibrated to actually represent probabilities of class membership. **In other words, the ROC curve and the AUC would be identical even if your predicted probabilities ranged from 0.9 to 1 instead of 0 to 1, as long as the ordering of observations by predicted probability remained the same.** All the AUC metric cares about is how well your classifier separated the two classes, and thus it is said to only be sensitive to rank ordering. You can think of AUC as representing the probability that a classifier will rank a randomly chosen positive observation higher than a randomly chosen negative observation, and thus it is a useful metric even for datasets with highly unbalanced classes.\n",
        "\n",
        "The second note is that ROC curves can be extended to classification problems with three or more classes using what is called a \"one versus all\" approach. That means if you have three classes, you would create three ROC curves. In the first curve, you would choose the first class as the positive class, and group the other two classes together as the negative class. In the second curve, you would choose the second class as the positive class, and group the other two classes together as the negative class. And so on.\n",
        "\n",
        "Finally, you might be wondering how you should set your classification threshold, once you are ready to use it to predict out-of-sample data. That's actually more of a business decision, in that you have to decide whether you would rather minimize your False Positive Rate or maximize your True Positive Rate. In our journal example, it's not obvious what you should do. But let's say your classifier was being used to predict whether a given credit card transaction might be fraudulent and thus should be reviewed by the credit card holder. The business decision might be to set the threshold very low. That will result in a lot of false positives, but that might be considered acceptable because it would maximize the true positive rate and thus minimize the number of cases in which a real instance of fraud was not flagged for review.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe__BiyUoAZN"
      },
      "source": [
        "### Baye's Theorem in ML\n",
        "\n",
        "[link](https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/)\n",
        "\n",
        "The article describes a cancer testing scenario:\n",
        "\n",
        "- 1% of women have breast cancer (and therefore 99% do not).\n",
        "- 80% of mammograms detect breast cancer when it is there (and therefore 20% miss it).\n",
        "- 9.6% of mammograms detect breast cancer when it’s not there (and therefore 90.4% correctly return a negative result).\n",
        "\n",
        "Put in a table, the probabilities look like this:\n",
        "\n",
        "![](https://betterexplained.com/wp-content/uploads/math/bayes/bayes_table.png)\n",
        "\n",
        "- 1% of people have cancer\n",
        "- If you already have cancer, you are in the first column. There’s an 80% chance you will test positive. There’s a 20% chance you will test negative.\n",
        "- If you don’t have cancer, you are in the second column. There’s a 9.6% chance you will test positive, and a 90.4% chance you will test negative.\n",
        "\n",
        "Now suppose you get a positive test result. What are the chances you have cancer? 80%? 99%? 1%?\n",
        "\n",
        "Here’s how I think about it:\n",
        "\n",
        "- Ok, we got a positive result. It means we’re somewhere in the top row of our table. Let’s not assume anything — it could be a true positive or a false positive.\n",
        "- The chances of a true positive = chance you have cancer * chance test caught it = 1% * 80% = .008\n",
        "- The chances of a false positive = chance you don’t have cancer * chance test caught it anyway = 99% * 9.6% = 0.09504\n",
        "\n",
        "The table looks like this:\n",
        "\n",
        "![](https://betterexplained.com/wp-content/uploads/2017/01/bayes_table_computed_new_png.png)\n",
        "\n",
        "And what was the question again? Oh yes: what’s the chance we really have cancer if we get a positive result. The chance of an event is the number of ways it could happen given all possible outcomes:\n",
        "\n",
        "![](https://betterexplained.com/wp-content/plugins/wp-latexrender/pictures/ee5390894367d1b1711d23786f6eca84.png)\n",
        "\n",
        "The chance of getting a real, positive result is .008. The chance of getting any type of positive result is the chance of a true positive plus the chance of a false positive (.008 + 0.09504 = .10304).\n",
        "\n",
        "So, our chance of cancer is .008/.10304 = 0.0776, or about 7.8%.\n",
        "\n",
        "#### Bayes’ Theorem\n",
        "\n",
        "We can turn the process above into an equation, which is Bayes’ Theorem. It lets you take the test results and correct for the “skew” introduced by false positives. You get the real chance of having the event. Here’s the equation:\n",
        "\n",
        "![](https://betterexplained.com/topics/Bayes_Theorem.png)\n",
        "\n",
        "And here’s the decoder key to read it:\n",
        "\n",
        "- Pr(H|E) = Chance of having cancer (H) given a positive test (E). This is what we want to know: How likely is it to have cancer with a positive result? In our case it was 7.8%.\n",
        "- Pr(E|H) = Chance of a positive test (E) given that you had cancer (H). This is the chance of a true positive, 80% in our case.\n",
        "- Pr(H) = Chance of having cancer (1%).\n",
        "- Pr(not H) = Chance of not having cancer (99%).\n",
        "- Pr(E|not H) = Chance of a positive test (E) given that you didn’t have cancer (not H). This is a false positive, 9.6% in our case.\n",
        "\n",
        "For the problem above:\n",
        "\n",
        "P(H|E) = (0.8x0.1)/(0.8x0.1 + 0.096x0.99) = 7.8% (same result)\n",
        "\n",
        "It all comes down to the chance of a true positive divided by the chance of any positive. We can simplify the equation to:\n",
        "\n",
        "![](https://betterexplained.com/wp-content/plugins/wp-latexrender/pictures/abb9243a6a7d838ec02cc5f9713e5604.png)\n",
        "\n",
        "> Pr(E) tells us the chance of getting any positive result, whether a true positive in the cancer population (1%) or a false positive in the non-cancer population (99%). In acts like a weighting factor, adjusting the odds towards the more likely outcome.\n",
        "\n",
        "Forgetting to account for false positives is what makes the low 7.8% chance of cancer (given a positive test) seem counter-intuitive. Thank you, normalizing constant, for setting us straight!\n",
        "\n",
        "#### Bayesian Spam Filtering\n",
        "\n",
        "One clever application of Bayes’ Theorem is in spam filtering. We have\n",
        "\n",
        "- Event A: The message is spam.\n",
        "- Test X: The message contains certain words (X)\n",
        "\n",
        "Plugged into a more readable formula (from Wikipedia):\n",
        "\n",
        "![](https://betterexplained.com/wp-content/plugins/wp-latexrender/pictures/da59f0597e5b97578af81ba86fe6dc3e.png)\n",
        "\n",
        "Bayesian filtering allows us to predict the chance a message is really spam given the “test results” (the presence of certain words). Clearly, words like “viagra” have a higher chance of appearing in spam messages than in normal ones.\n",
        "\n",
        "Spam filtering based on a blacklist is flawed — it’s too restrictive and false positives are too great. But Bayesian filtering gives us a middle ground — we use probabilities. **As we analyze the words in a message, we can compute the chance it is spam (rather than making a yes/no decision). If a message has a 99.9% chance of being spam, it probably is. As the filter gets trained with more and more messages, it updates the probabilities that certain words lead to spam messages.** Advanced Bayesian filters can examine multiple words in a row, as another data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbesVAtvg8d"
      },
      "source": [
        "### What’s the difference between Type I and Type II error?\n",
        "\n",
        "Type I error is a false positive, while Type II error is a false negative. \n",
        "\n",
        "A clever way to think about this is to think of Type I error as telling a\n",
        "man he is pregnant, while Type II error means you tell a pregnant\n",
        "woman she isn’t carrying a baby."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ-clSFSni3p"
      },
      "source": [
        "### Data Leakage in Machine Learning\n",
        "\n",
        "[link](https://machinelearningmastery.com/data-leakage-machine-learning/)\n",
        "\n",
        "[link](https://www.kaggle.com/alexisbcook/data-leakage)\n",
        "\n",
        "Data leakage is when information from outside the training dataset is > used to create the model.\n",
        "\n",
        "> Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed.\n",
        "\n",
        "Two good techniques that you can use to minimize data leakage when developing predictive models are as follows:\n",
        "\n",
        "- Perform data preparation within your cross validation folds.\n",
        "- Hold back a validation dataset for final sanity check of your developed models.\n",
        "\n",
        "Generally, it is good practice to use both of these techniques.\n",
        "\n",
        "#### Perform Data Preparation Within Cross Validation Folds\n",
        "\n",
        "You can easily leak information when preparing your data for machine learning.\n",
        "\n",
        "\n",
        "he effect is overfitting your training data and having an overly optimistic evaluation of you models performance on unseen data.\n",
        "\n",
        "For example, if you normalize or standardize your entire dataset, then estimate the performance of your model using cross validation, you have committed the sin of data leakage.\n",
        "\n",
        "The data rescaling process that you performed had knowledge of the full distribution of data in the training dataset when calculating the scaling factors (like min and max or mean and standard deviation). This knowledge was stamped into the rescaled values and exploited by all algorithms in your cross validation test harness.\n",
        "\n",
        "**A non-leaky evaluation of machine learning algorithms in this situation would calculate the parameters for rescaling data within each fold of the cross validation and use those parameters to prepare the data on the held out test fold on each cycle.**\n",
        "\n",
        "\n",
        "#### Hold Back a Validation Dataset\n",
        "\n",
        "Another, perhaps simpler approach is to split your training dataset into train and validation sets, and store away the validation dataset.\n",
        "\n",
        "Once you have completed your modeling process and actually created your final model, evaluate it on the validation dataset.\n",
        "\n",
        "This can give you a sanity check to see if your estimation of performance has been overly optimistic and has leaked.\n",
        "\n",
        "There are two main types of leakage: target leakage and train-test contamination.\n",
        "\n",
        "\n",
        "#### Target leakage\n",
        "\n",
        "Target leakage occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions.\n",
        "\n",
        "An example will be helpful. Imagine you want to predict who will get sick with pneumonia. People take antibiotic medicines after getting pneumonia in order to recover. The raw data shows a strong relationship between those columns, but `took_antibiotic_medicine` is frequently changed after the value for `got_pneumonia` is determined. This is target leakage.\n",
        "\n",
        "The model would see that anyone who has a value of False for `took_antibiotic_medicine` didn't have pneumonia. Since validation data comes from the same source as training data, the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores.\n",
        "\n",
        "But the model will be very inaccurate when subsequently deployed in the real world, because even patients who will get pneumonia won't have received antibiotics yet when we need to make predictions about their future health.\n",
        "\n",
        "**To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.**\n",
        "\n",
        "\n",
        "#### Train-Test Contamination\n",
        "\n",
        "\n",
        "A different type of leak occurs when you aren't careful to distinguish training data from validation data.\n",
        "\n",
        "Recall that validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train-test contamination.\n",
        "\n",
        "For example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n",
        "\n",
        "After all, you incorporated data from the validation or test data into how you make predictions, so the may do well on that particular data even if it can't generalize to new data. This problem becomes even more subtle (and more dangerous) when you do more complex feature engineering.\n",
        "\n",
        "If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn pipelines. When using cross-validation, it's even more critical that you do your preprocessing inside the pipeline!\n",
        "\n",
        "\n",
        "#### Example\n",
        "\n",
        "In this example, you will learn one way to detect and remove target leakage.\n",
        "\n",
        "We will use a dataset about credit card applications and skip the basic data set-up code. The end result is that information about each credit card application is stored in a DataFrame X. We'll use it to predict which applications were accepted in a Series y.\n",
        "\n",
        "Since this is a small dataset, we will use cross-validation to ensure accurate measures of model quality.\n",
        "\n",
        "`Cross-validation accuracy: 0.980283`\n",
        "\n",
        "With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's uncommon enough that we should inspect the data more closely for target leakage.\n",
        "\n",
        "Here is a summary of the data, which you can also find under the data tab:\n",
        "\n",
        "- card: 1 if credit card application accepted, 0 if not\n",
        "- reports: Number of major derogatory reports\n",
        "- age: Age n years plus twelfths of a year\n",
        "- income: Yearly income (divided by 10,000)\n",
        "- share: Ratio of monthly credit card expenditure to yearly income\n",
        "- expenditure: Average monthly credit card expenditure\n",
        "- owner: 1 if owns home, 0 if rents\n",
        "- selfempl: 1 if self-employed, 0 if not\n",
        "- dependents: 1 + number of dependents\n",
        "- months: Months living at current address\n",
        "- majorcards: Number of major credit cards held\n",
        "- active: Number of active credit accounts\n",
        "\n",
        "\n",
        "A few variables look suspicious. For example, does expenditure mean expenditure on this card or on cards used before appying?\n",
        "\n",
        "At this point, basic data comparisons can be very helpful:\n",
        "\n",
        "```\n",
        "Fraction of those who did not receive a card and had no expenditures: 1.00\n",
        "Fraction of those who received a card and had no expenditures: 0.02\n",
        "```\n",
        "\n",
        "As shown above, everyone who did not receive a card had no expenditures, while only 2% of those who received a card had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this also seems to be a case of target leakage, where expenditures probably means expenditures on the card they applied for.\n",
        "\n",
        "Since share is partially determined by expenditure, it should be excluded too. The variables active and majorcards are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\n",
        "\n",
        "We would run a model without target leakage as follows:\n",
        "\n",
        "```\n",
        "# Drop leaky predictors from dataset\n",
        "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
        "X2 = X.drop(potential_leaks, axis=1)\n",
        "```\n",
        "\n",
        "This results in:\n",
        "\n",
        "`Cross-val accuracy: 0.830170`\n",
        "\n",
        "This accuracy is quite a bit lower, which might be disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse than that (in spite of its higher apparent score in cross-validation).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "[another great article on this topic](https://towardsdatascience.com/data-leakage-in-machine-learning-10bdd3eec742)\n",
        "\n",
        "\n",
        "#### Introduction\n",
        "\n",
        "When training a machine learning model, we normally aim for the model that scores the highest on some metric, such as accuracy. Naturally, then, when we train a model that appears to score very well on our validation or test data-set, we select it as a well-performing model and productionize/finalize it.\n",
        "\n",
        "However, have you ever encountered a situation in which a model performs well during testing, but fails to achieve the same level of performance during real-world usage? For example, has your model reached 99% accuracy during testing, but as soon as it is productionized and acts on real data, it fails to get anywhere near that level of performance?\n",
        "\n",
        "Such a discrepancy between test performance and real-world performance is often explained by a phenomenon called data leakage.\n",
        "\n",
        "Data leakage refers to a mistake make by the creator of a machine learning model in which they accidentally share information between the test and training data-sets. Typically, when splitting a data-set into testing and training sets, the goal is to ensure that no data is shared between the two. This is because the test set’s purpose is to simulate real-world, unseen data. However, when evaluating a model, we do have full access to both our train and test sets, so it is up to us to ensure that no data in the training set is present in the test set.\n",
        "Data leakage often results in unrealistically-high levels of performance on the test set, because the model is being ran on data that it had already seen — in some capacity — in the training set. The model effectively memorizes the training set data, and is easily able to correctly output the labels/values for those test data-set examples. Clearly, this is not ideal, as it misleads the person evaluating the model. When such a model is then used on truly unseen data, performance will be much lower than expected.\n",
        "\n",
        "#### Causes of Data Leakage\n",
        "\n",
        "Now I will mention some common causes of data leakage. It is important to avoid these situations when training models of your own. In general, you should avoid doing anything to your training set that involves having knowledge of the test set.\n",
        "\n",
        "##### Pre-processing\n",
        "\n",
        "\n",
        "A very common error that people make is to leak information in the data pre-processing step of machine learning. It is essential that these transformations only have knowledge of the training set, even though they are applied to the test set as well. For example, if you decide that you want to run PCA as a pre-processing step, you should fit your PCA model on only the training set. Then, to apply it to your test set, you would only call its transform method (in the case of a scikit-learn model) on the test set. If, instead, you fit your pre-processor on the entire data-set, you will leak information from the test set, since the parameters of the pre-processing model will be fitted with knowledge of the test set.\n",
        "\n",
        "##### Duplicates\n",
        "\n",
        "Another error, which is especially common when your data-set comes from noisy, real-world data, is data duplication. This occurs when your data-set contains several points with identical or near-identical data. For example, if your data-set contained user messages on a messaging platform, duplicates may correspond to spammers who send the same message to many users. In this situation, you may experience data leakage simply due to the fact that your train and test set may contain the same data point, even though they may correspond to different observations. This can be fixed by de-duplicating your data-set prior to splitting into train and test sets. You can either do this by removing exact duplicates, or by using a fuzzy matching method (such as via edit distance for text data) to remove approximate matches.\n",
        "\n",
        "##### Temporal Data (implicit leakage)\n",
        "\n",
        "Even when you are not explicitly leaking information, you may still experience data leakage if there are dependencies between your test and train set. A common example of this occurs with temporal data, which is data where time is a relevant factor, such as time-series data. Consider the following toy example: your training set consists of two data-points A and C, and your training set consists of one data-point, B. Now, suppose that the temporal ordering of these data-points is A →B →C. Here, we have most likely created a data leakage simply through the way we created our training and test sets. By training on point C and testing on point B, we created an unrealistic situation in which we train our model on future knowledge, relative to the test set’s point in time. Therefore, we have leaked information, as in a real-world scenario, our model clearly would not have any knowledge of the future. In order to fix this problem, you should ensure that your test-train split is also split across time. So, everything in your training set should occur before everything in your test set. This creates a much more realistic training situation, and allows you to properly evaluate your model as if it were acting on incoming real-world data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvkcUALF_tRv"
      },
      "source": [
        "### k-Fold Cross-Validation\n",
        "\n",
        "[link](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
        "\n",
        "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
        "\n",
        "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
        "\n",
        "The general procedure is as follows:\n",
        "\n",
        "\n",
        "1. Shuffle the dataset randomly.\n",
        "2. Split the dataset into k groups\n",
        "3. For each unique group:\n",
        "  - Take the group as a hold out or test data set\n",
        "  - Take the remaining groups as a training data set\n",
        "  - Fit a model on the training set and evaluate it on the test set\n",
        "  - Retain the evaluation score and discard the model\n",
        "4. Summarize the skill of the model using the sample of model evaluation scores\n",
        "\n",
        "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.\n",
        "\n",
        "> It is also important that any preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.\n",
        "\n",
        "\n",
        "The results of a k-fold cross-validation run are often summarized with the mean of the model skill scores. It is also good practice to include a measure of the variance of the skill scores, such as the standard deviation or standard error.\n",
        "\n",
        "#### Configuration of k\n",
        "\n",
        "The k value must be chosen carefully for your data sample.\n",
        "\n",
        "A poorly chosen value for k may result in a mis-representative idea of the skill of the model, such as a score with a high variance (that may change a lot based on the data used to fit the model), or a high bias, (such as an overestimate of the skill of the model).\n",
        "\n",
        "Three common tactics for choosing a value for k are as follows:\n",
        "\n",
        "- Representative: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
        "- k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
        "- k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.\n",
        "\n",
        "> To summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n",
        "\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```\n",
        "# scikit-learn k-fold cross-validation\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "# data sample\n",
        "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
        "# prepare cross validation\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "# enumerate splits\n",
        "for train, test in kfold.split(data):\n",
        "\tprint('train: %s, test: %s' % (data[train], data[test]))\n",
        "  \n",
        "  \n",
        "Output:\n",
        "train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\n",
        "train: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\n",
        "train: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n",
        "```\n",
        "\n",
        "#### Variations on Cross-Validation\n",
        "\n",
        "There are a number of variations on the k-fold cross validation procedure.\n",
        "\n",
        "Three commonly used variations are as follows:\n",
        "\n",
        "- Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
        "- LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n",
        "- Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
        "- Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1i52yaNnlBy"
      },
      "source": [
        "### Micro Average vs Macro average in Multiclass classification\n",
        "\n",
        "We know that F1 score is basically a harmonic mean of Precision and Recall and it is calculated for each class. However when we need a single number that combines the score for each class and we want to take that number as a measure of performance for our system, how do we calculate that? Do we simply take the average of the F1 scores of each class\n",
        "\n",
        "> Excellent explanation by pythiest: https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n",
        "\n",
        "\n",
        "Micro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).\n",
        "\n",
        "To illustrate why, take for example precision $Pr=\\frac{TP}{(TP+FP)}$\n",
        "Let's imagine you have a One-vs-All (there is only one correct class output per example) multi-class classification system with four classes and the following numbers when tested:\n",
        "\n",
        "```\n",
        "Class A: 1 TP and 1 FP\n",
        "Class B: 10 TP and 90 FP\n",
        "Class C: 1 TP and 1 FP\n",
        "Class D: 1 TP and 1 FP\n",
        "```\n",
        "\n",
        "You can see easily that $Pr_A = Pr_C = Pr_D = 0.5$ whereas $Pr_B=0.1$\n",
        "\n",
        "- A macro-average will then compute: $Pr=\\frac{0.5+0.1+0.5+0.5}{4}=0.4$\n",
        "\n",
        "- A micro-average will compute: $Pr=\\frac{1+10+1+1}{2+100+2+2}=0.123$\n",
        "\n",
        "These are quite different values for precision. Intuitively, in the macro-average the \"good\" precision (0.5) of classes A, C and D is contributing to maintain a \"decent\" overall precision (0.4). While this is technically true (across classes, the average precision is 0.4), it is a bit misleading, since a large number of examples are not properly classified. These examples predominantly correspond to class B, so they only contribute 1/4 towards the average in spite of constituting 94.3% of your test data. The micro-average will adequately capture this class imbalance, and bring the overall precision average down to 0.123 (more in line with the precision of the dominating class B (0.1)).\n",
        "\n",
        "If class imbalance is known to be an issue, there are several ways around it. One is to report not only the macro-average, but also its standard deviation (for 3 or more classes). Another is to compute a weighted macro-average, in which each class contribution to the average is weighted by the relative number of examples available for it. In the above scenario, we obtain:\n",
        "\n",
        "\n",
        "$Pr_{macro-stdev}=0.173$\n",
        "\n",
        "The large standard deviation (0.173) already tells us that the 0.4 average does not stem from a uniform precision among classes\n",
        "\n",
        "\n",
        "```\n",
        "wt_classA = 2/106 = 0.0189\n",
        "wt_classB = 100/106 = 0.9433\n",
        "wt_classC = 2/106 = 0.0189\n",
        "wt_classD = 2/106 = 0.0189\n",
        "```\n",
        "\n",
        "$Pr_{macro-weighted}={0.0189·0.5+0.943·0.1+0.0189·0.5+0.0189·0.5}={0.009+0.094+0.009+0.009}=0.123$\n",
        "\n",
        "The weighted macro-average, is in essence is another way of computing the micro-average.\n",
        "\n",
        "Why is this?\n",
        "\n",
        "Lets assume:\n",
        "\n",
        "For Class A : TP: t_A | FP: f_A\n",
        "\n",
        "For Class B : TP: t_B | FP: f_B\n",
        "\n",
        "For Class C : TP: t_C | FP: f_C\n",
        "\n",
        "For Class D : TP: t_D | FP: f_D\n",
        "\n",
        "Total (T): t_A + f_A + t_B + f_B + t_C + f_C + t_D + f_D\n",
        "\n",
        "micro-average as described = $\\frac{t_A + t_B + t_C + t_D}{T}$\n",
        "\n",
        "Now weight_A = $\\frac{t_A + f_A}{T}$\n",
        "\n",
        "Similarly, weight_B = $\\frac{t_B + f_B}{T}$\n",
        "\n",
        "and so on..\n",
        "\n",
        "precision_A = $\\frac{t_A }{t_A + f_A}$\n",
        "\n",
        "Similarly, precision_B = $\\frac{t_B }{t_B + f_B}$\n",
        "\n",
        "and so on..\n",
        "\n",
        "macro-weighted (for A) = weight_A *  precision_A = $\\frac{t_A + f_A}{T}  \\times \\frac{t_A }{t_A + f_A}$ = $\\frac{t_A}{T}$\n",
        "\n",
        "Similarly, macro-weighted (for B) = $\\frac{t_B}{T}$\n",
        "\n",
        "macro-weighted = macro-weighted (for A) + macro-weighted (for B) + macro-weighted (for D) + macro-weighted (for D)\n",
        "= $\\frac{t_A}{T} + \\frac{t_B}{T} + \\frac{t_C}{T} + \\frac{t_D}{T}$ = $\\frac{t_A + t_B + t_C + t_D}{T}$ = micro-average#\n",
        "\n",
        "Hence `micro-average == macro-average (weighted)`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl6pKbX0nln_"
      },
      "source": [
        "### NDCG : Evaluation for Recommender Systems\n",
        "\n",
        "> https://towardsdatascience.com/evaluate-your-recommendation-engine-using-ndcg-759a851452d1\n",
        "\n",
        "One of the key challenges when developing or enhancing a recommendation engine is evaluating its impact. Though we are sure about the positive impact, there is a need to quantify the same. This quantification not only facilitates stakeholder communication but also serves as a benchmark for future enhancements.\n",
        "\n",
        "__Normalized Discounted Cumulative Gain or NDCG is a measure of ranking quality.__\n",
        "\n",
        "The first question which popped up in my mind after reading about this was How a measure of ranking quality will be appropriate for evaluating the recommendation engine?\n",
        "\n",
        "A recommendation engine recommends a set of documents from a superset which the engine finds to be most relevant to the user. In that sense, a recommendation engine is simply performing a task of document retrieval. Thus, we can assess a recommendation engine using NDCG.\n",
        "\n",
        "To understand NDCG, we need to understand its predecessors: Cumulative Gain(CG) and Discounted Cumulative Gain(DCG). Also, we need to keep the following assumption in mind:\n",
        "\n",
        "> The highly relevant documents are more useful than moderately relevant documents, which are in turn more useful than irrelevant documents.\n",
        "\n",
        "\n",
        "#### Cumulative Gain(CG)\n",
        "\n",
        "Every recommendation has a relevance score associated with it. Cumulative Gain is the sum of all the relevance scores in a recommendation set.\n",
        "\n",
        "![](https://miro.medium.com/max/404/1*YSM96w8n9m1zD16cCsAckg.png)\n",
        "\n",
        "Thus, CG for ordered recommendation set A with document relevance scores will be:\n",
        "\n",
        "![](https://miro.medium.com/max/314/1*2lIELxGGNn0duVrMPazq3g.png)\n",
        "\n",
        "#### Discounted Cumulative Gain(DCG)\n",
        "\n",
        "There is a drawback with Cumulative Gain. Consider the following two ordered recommendation sets with relevance scores of individual documents.\n",
        "\n",
        "![](https://miro.medium.com/max/326/1*aij2Wg3mDR5tgdFILD_z4w.png)\n",
        "\n",
        "We know that Set B is better than Set A as it is recommending in decreasing order of relevance, but as per the Cumulative Gain, both sets are equally good. What exactly lacking is the use of position along with relevance scores. DCG fills this gap. The computation involves discounting the relevance score by dividing it with the log of the corresponding position.\n",
        "\n",
        "![](https://miro.medium.com/max/215/1*PbLqFhWQ2Vh1hCHnPkCAIA.png)\n",
        "\n",
        "Alternatively, it can also be computed using the below expression.\n",
        "\n",
        "![](https://miro.medium.com/max/228/1*9o1wEjutDhfxCWsNc5DpjQ.png)\n",
        "\n",
        "This second expression penalizes heavily as compared to the first one if the document with higher relevance is ranked lower. Depending on the application you can choose either one of the expressions to compute the DCG and NDCG.\n",
        "\n",
        "If the relevance scores are binary i.e. either 0 or 1, then the two expressions yield the same result.\n",
        "\n",
        "\n",
        "Let us compute the DCG for both ordered sets using the first expression.\n",
        "\n",
        "![](https://miro.medium.com/max/663/1*9l9SiZDnkSKLHVxgxO_mnQ.png)\n",
        "\n",
        "The DCG results for the above example is aligned with our intuition. Set B is better than Set A.\n",
        "\n",
        "\n",
        "#### Normalized Discounted Cumulative Gain(NDCG)\n",
        "\n",
        "DCG seems a good measure at first as it takes position significance into account. However, it is still not complete. Depending on various factors, the number of recommendations served may vary for every user. The greater the number of recommendations the greater will be the DCG as it is an additive measure. We need a score which has a proper upper and lower bounds so that we can take a mean across all the recommendations score to report a final score. NDCG brings in this normalization.\n",
        "\n",
        "For every recommendation set, to compute NDCG, we need to compute :\n",
        "\n",
        "1. DCG of the recommended order\n",
        "2. DCG of the ideal order(iDCG).\n",
        "\n",
        "\n",
        "NDCG is then the ratio of DCG of recommended order to DCG of ideal order.\n",
        "\n",
        "![](https://miro.medium.com/max/179/1*t_HZ7L75U3VFCwUsu3ax6g.png)\n",
        "\n",
        "> This ratio will always be in the range `[0,1]`.\n",
        "\n",
        "\n",
        "Consider the following ordered recommendation set served to one of the users.\n",
        "\n",
        "![](https://miro.medium.com/max/445/1*vKlk14pRdQ_UNOnw1oSOPw.png)\n",
        "\n",
        "The ideal order for this recommendation set will be:\n",
        "\n",
        "![](https://miro.medium.com/max/313/1*3sGaiio-px4nUuSh4n4HqQ.png)\n",
        "\n",
        "The corresponding DCG scores using the first expression:\n",
        "\n",
        "![](https://miro.medium.com/max/658/1*2M-A2BSvnU_z3X-_LebZiA.png)\n",
        "\n",
        "Thus, the NDCG for this recommendation set will be:\n",
        "\n",
        "![](https://miro.medium.com/max/309/1*VaDs4jG6GM68e3oaGX14jw.png)\n",
        "\n",
        "> To evaluate a recommendation engine, compute a mean of NDCG for the recommendations served to the test set of the users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QwG6B2Znlfg"
      },
      "source": [
        "### The Curse of Dimensionality\n",
        "\n",
        "> https://www.youtube.com/watch?v=DyxQUHz4jWg\n",
        "\n",
        "The COD is basically a problem in DS when we increase the no of varibales, a no of issues start creeping into the ML algos\n",
        "\n",
        "Let us consider the KNN algorithm\n",
        "\n",
        "![](https://i.imgur.com/G8SQhNz.png)\n",
        "\n",
        "Here we have 2 classes (red and blue)\n",
        "\n",
        "First let us assume that each data point has 1 dimension\n",
        "\n",
        "![](https://i.imgur.com/INXWrOI.png)\n",
        "\n",
        "We can visualize that there are 2 distinct clusters here, so the algo should be able to pick this up\n",
        "\n",
        "To simulate how \"easy\" it is for the algo to understand the separation bw points let us calculate the pairwise Euclidean dist for each pair of points\n",
        "\n",
        "And let us plot these distances in the histogram:\n",
        "\n",
        "![](https://i.imgur.com/EkPulTt.png)\n",
        "\n",
        "Not the very high peak around 0. This is because points in same class are quite close together. So KNN works quite well here\n",
        "\n",
        "This similar tren follows for 2D as well:\n",
        "\n",
        "![](https://i.imgur.com/cyXPenC.png)\n",
        "\n",
        "\n",
        "The peak around 0 signifies that there are clusters in which points are quite close\n",
        "\n",
        "As we inc the dimensions however this trend changes and the peak around 0 starts to reduce:\n",
        "\n",
        "![](https://i.imgur.com/2zz9h9Y.png)\n",
        "\n",
        "![](https://i.imgur.com/U9R858C.png)\n",
        "\n",
        "Very few pairs of data points are close to each other. Most pairs of data points are fairly far apart. The idea of the data points close to me are of the same class thus starts to break down in higher dimensions\n",
        "\n",
        "So there are not really many neighbors in high dimensional space (using the Euclidean dist metric). Every data pt is kind of very far from every other data pt\n",
        "\n",
        "#### Another perspective: Number of samples needed\n",
        "\n",
        "Say our variables/features are binary in nature\n",
        "\n",
        "Also assume we need 10 samples per unique combination of variables\n",
        "\n",
        "```\n",
        "1 binary var -> 2 combos -> 20 samples needed \n",
        "2 binary vars -> 4 combos -> 40 samples needed\n",
        "3 binary var -> 2^3 = 8 combos -> 80 samples needed\n",
        "...\n",
        "k binary var -> 2^k combos -> 10 x 2^k samples needed\n",
        "```\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/H8cS6Qv.png)\n",
        "\n",
        "\n",
        "We can see that to get the same level of statistical robustness with gretaer no of vars, we need exponential no of more samples\n",
        "\n",
        "For 17 vars we need over a million samples\n",
        "For 20 vars we need 10 million samples\n",
        "\n",
        "The more vars we consider in our problem exponentially more the no of data pts we need\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Y5gXgfnlTw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZIci1uWnlHO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tMe7dK_a2MG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}